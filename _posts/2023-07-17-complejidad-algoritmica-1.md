---
title: Complejidad algor칤tmica (parte I)
date: 2023-07-17T08:00:00+01:00
author: Carlos Buchart
layout: post
permalink: /2023/07/17/complejidad-algoritmica-1
image: /assets/images/featured/algorithm_complexity.jpg
excerpt: Introducci칩n al concepto de complejidad algor칤tmica, su impacto en el desarrollo y algunas consideraciones iniciales
categories: algorithm performance
---
## Introducci칩n

Sin entrar a filosofar demasiado, podr칤amos decir que para que un determinado c칩digo pueda considerarse bueno, hacen falta cinco cosas:

- Hacer lo que tiene que hacer, es decir, cumplir con los requerimientos.
- No hacer lo que no debe hacer (no tener errores, ser seguro, ser fiable).
- Hacerlo eficientemente, con el menor consumo de recursos posible.
- Acoplarse correctamente al resto del sistema, sin interferir con otras aplicaciones.
- Ser entendible tanto por el equipo actual como por el del futuro (expresividad y documentaci칩n).

As칤 como en otras ocasiones hemos hablado mucho del 칰ltimo punto, hoy (y en futuras entregas) lo haremos del tercero: eficiencia, y m치s espec칤ficamente de un aspecto del rendimiento llamado _complejidad algor칤tmica_. Aunque este tema ha sido abordado por numerosos autores de una forma mucha m치s profunda de lo que lo haremos ac치, el objetivo de estas entradas es introducir el concepto y su importancia, as칤 como dar ejemplos y gu칤as r치pidas de uso que nos permitan sacar provecho del mismo en nuestros proyectos.

## Complejidad algor칤tmica

El concepto de complejidad algor칤tmica se refiere a c칩mo se comporta un determinado c칩digo cuando el conjunto de datos sobre el que opera crece (se dice que su tama침o _tiende a infinito_). Es decir, nos habla principalmente de la _escalabilidad_ del c칩digo, y tambi칠n, aunque de forma indirecta, de su eficiencia.

La complejidad algor칤tmica suele evaluarse considerando dos aspectos: el temporal (tiempo de ejecuci칩n) y el espacial (memoria requerida). Aunque trataremos de abordar ambos a lo largo de estas entregas, nos centraremos en el an치lisis de tiempo, pudi칠ndose tomar la teor칤a y aplicarla directamente al espacial la gran mayor칤a de las veces.

Para realizar este an치lisis necesitaremos una forma de indicar la complejidad obtenida, y lo haremos utilizando la _notaci칩n asint칩tica_, m치s espec칤ficamente de la O grande (aunque existen otros tipos).

## Notaci칩n asint칩tica O grande

Esta notaci칩n indica una cota m치xima en la complejidad de un algoritmo. Indica, _grosso modo_, c칩mo es el comportamiento de un algoritmo (en tiempo o espacio) a medida que crece el conjunto de datos. No se expresa en unidades de tiempo (o de memoria) espec칤ficas, ni siquiera en t칠rminos de instrucciones, ya que dependen de muchos factores (compilador, _flags_ utilizados, arquitectura, hardware disponible, entorno, etc).

Tampoco es un an치lisis detallado del n칰mero de operaciones que un algoritmo realiza, o de los bytes que consume, sino un resumen de su tendencia principal. As칤, un algoritmo que sume elemento a elemento dos vectores, y otro que realice 514 operaciones por cada par de elementos, tendr치 la misma notaci칩n O grande (en este caso O(n), pero eso lo veremos en breve). 쯇or qu칠? Porque a medida que el conjunto de datos crece, los detalles de implementaci칩n tienen cada vez menos impacto frente al comportamiento general del mismo. (Obviamente, esto no quita que a la hora de comparar exhaustivamente dos algoritmos o implementaciones no debamos tomar en cuenta estos detalles, pero en esta serie nos centraremos en lo antes expuesto.)

La notaci칩n O grande busca pues describir, con sencillez, este comportamiento, de forma que podamos hacernos una idea del rendimiento de un algoritmo y poder realizar comparaciones entre distintas soluciones. Algunos de los tipos principales son (en orden de _mejor_ a _peor_):

- O(1): constante (el tiempo o espacio requerido no se ve afectado por el tama침o del conjunto de datos). Ejemplos son el acceso a un arreglo o vector de datos, consultas a tablas _hash_, y b칰squeda de m치ximo o m칤nimo en un conjunto ordenado.
- O(log n): logar칤tmico (normalmente se descartan secciones completas del conjunto de datos durante el procesamiento). El tipo de algoritmo m치s conocido de este orden son las b칰squedas dicot칩micas (o binarias).
- O(n): lineal (seguramente el caso m치s trivial, recorrer los datos un n칰mero constante de veces). Se identifican r치pidamente por la presencia de un bluce _for_ del tipo `for (size_t i = 0; i < N; ++i)` (o variantes).
- O(n log n): cuasi-lineal. La gran mayor칤a de algoritmos de ordenaci칩n eficientes (tales como _quick-sort_) tienen esta complejidad.
- O(n^2^): cuadr치tico (recorrer el conjunto de datos por cada elemento del mismo). Suelen consistir en un par de bucles anidados y, en muchos casos, corresponden a la versi칩n m치s directa (y no optimizada) de un algoritmo.
- O(n^3^): c칰bico. An치logamente al cuadr치tico, encontramos tres bucles anidados. Estos casos son raros de ver de forma directa y suelen aparecer disfrazados como la aplicaci칩n, a modo de subrutina, de un algoritmo cuadr치tico a cada elemento de un conjunto de datos.
- O(2^n^): exponencial. Un ejemplo son las b칰squedas de caminos 칩ptimos por fuerza bruta.

## Rendimiento promedio, mejor y peor caso

Lo m치s normal es medir el rendimiento de un algoritmo en los casos m치s comunes. A칰n as칤, muchos algoritmos se comportan de forma m치s eficiente en determinadas situaciones. Por ejemplo, algunos algoritmos de ordenanamiento (entre ellos el _infame_ algoritmo de la burbuja) pueden llegar a ser O(n) sobre conjuntos previamente ordenados. As칤 mismo, puede pasar que haya casos en los que el rendimiento decaiga dram치ticamente (por poner otro ejemplo interesante, el _quick-sort_ puede llegar a ser O(n^2^) si el conjunto est치 ordenado de forma inversa).

El conocimiento del comportamiento del algoritmo en todos estos casos nos proporcionar치 una gu칤a 칰til para elegir el m치s acorde a nuestras necesidades.

## Ejemplo

Para entenderlo mejor, veamos c칩mo se comportar칤an un grupo de funciones, todas calculando el mismo resultado pero cada una con una complejidad media diferente. Ya mencionamos anteriormente que la complejidad algor칤tmica no est치 asociada a tiempos espec칤ficos, pero ilustrar con algunos n칰meros reales siempre ayuda a entender mejor el concepto. Supongamos que para el caso b치sico (N=1) todas las variantes tardasen 0,1us (venga, un tiempo a primera vista _rid칤culamente peque침o_). Ahora, _midamos_ (desde un punto de vista te칩rico y simplista) cu치nto tardar칤an en ejecutarse estos algoritmos para N=100, N=10.000 y N=1.000.000:

|Complejidad|1|100|10.000|1.000.000|
|--|--|--|--|--|
|O(1)|0,1us|0,1us|0,1us|0,1us|
|O(log n)|0,1us|0,6us|1,3us|2us|
|O(n)|0,1us|10us|1ms|100ms|
|O(n log n)|0,1us|66us|13,3ms|2s|
|O(n^2^)|0,1us|1ms|10s|27,8h|
|O(n^3^)|0,1us|100ms|27,8h|3.171y|
|O(2^n^)|0,1us|游깳|游뱚|游뱚|

Nota: En este caso el algoritmo exponencial no nos servir칤a m치s que para conjunto de unas pocas unidades

Aunque pareciese que incluso los cuatro primeros tienen un rendimiento m치s que decente, tenemos que ponerlos en contexto. Para operaciones que se realizan una 칰nica vez, o muy espor치dicamente, tiempos de hasta unos pocos segundos pueden ser aceptables (guardar un fichero, la generaci칩n de miniaturas de un 치lbum de fotos, preparar un documento para su impresi칩n, precalcular tablas de valores). Por otro lado, si la operaci칩n debe ser realizada continuamente, o forma parte de un flujo de trabajo m치s largo, es probable que se convierta en nuestro cuello de botella y debamos buscar una alternativa.

Imaginemos que esta funci칩n es la encargada de calcular la colisi칩n entre el personaje de un videojuego y su entorno, y donde N es la cantidad de pol칤gonos en la escena. Si queremos un juego fluido deber칤amos entonces realizar este c치lculo un m칤nimo de 60 veces por segundo. As칤, si tenemos 10.000 pol칤gonos (algo bastante flojo hoy en d칤a), podemos aproximar el tiempo requerido:

||Tiempo por fotograma|60Hz|
|--|--|--|
|O(1)|0,1us|6us|
|O(log n)|1,3us|78us|
|O(n)|1ms|60ms|
|O(n log n)|13,3ms|798ms|

Vemos que el algoritmo O(n log n) se queda atr치s ya que consume casi todo el tiempo disponible en un segundo, y a칰n quedan otras tareas por hacer (IA, renderizado, sonido, comunicaciones...). Pero es que aunque se pusiese en un hilo dedicado, la detecci칩n de colisiones suele ser un c치lculo bloqueante de otras tareas, tales como interacci칩n con objetos, recibir da침o, restringir el movimiento. As칤 que incluso en el caso del O(n) estar칤amos consumiendo el 6% de nuestro valioso tiempo en esto antes de poder proseguir con otros c치lculos. Por 칰ltimo, suponer un escenario de s칩lo 10.000 pol칤gonos es, hoy en d칤a, hablar de un juego bastante sencillote. En entornos m치s exigentes (m치s de 1 mill칩n de pol칤gonos), la soluci칩n de orden lineal se mostrar칤a inficiente tambi칠n.

## Complejidad espacial

La tabla anterior mostr칩 la eficiencia de ejecuci칩n de un algoritmo. A la hora de hablar de complejidad espacial, tenemos que hacer hincapi칠 en que la gran mayor칤a de las veces se refiere al espacio requerido _por las estructuras auxiliares_, no por el conjunto de datos en s칤 que, obviamente, tendr치 que contener los datos que necesite (dejaremos de lado t칠cnicas de compresi칩n o de control de redundancias).

As칤 pues, imaginemos que tenemos una colecci칩n de objectos de clase `C`, donde cada uno ocupa 20 bytes y, para simplificar, asumamos que la alineaci칩n de memoria es siempre perfecta. Dicha colecci칩n debe ser procesada por diversos algoritmos, cada uno con una complejidad espacial diferente (no pasar칠 de O(N^2^), ya que suele ser el peor caso asociado). Para ilustrar el caso haremos los c치lculos suponiendo un _overhead_ de un objeto auxiliar (20B):

||1|1.000|1.000.000|
|--|--|--|--|
|O(1)|20B|20B|20B|
|O(log n)|20B|200B|400B|
|O(n)|20B|20KB|20MB|
|O(n log n)|20B|4MB|8GB|
|O(n^2^)|20B|20MB|20PB|

Vemos claramente c칩mo no suelen ser viables algoritmos que requieren m치s de O(n) espacio adicional. Esto sin entrar en detalles tales como el tiempo que conlleva la reserva de memoria ni el patr칩n de accesos a todos los datos (cach칠).

## Conclusiones

En esta primera entrega hemos expuesto las nociones de la complejidad algor칤tmica: notaci칩n O grande, complejidad temporal y espacial; y mostrado su impacto mediante ejemplos realistas.

Como gu칤a r치pida, en general debemos evitar cualquier algoritmo de orden cuadr치tico y superior en aquellos escenarios donde el conjunto de datos sea grande. En una entrega futura detallaremos la complejidad de algunos algoritmos conocidos as칤 como diversas t칠cnicas de optimizaci칩n que podemos utilizar.
